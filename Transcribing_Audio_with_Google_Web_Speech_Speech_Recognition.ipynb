{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEIrSzDXDHmYveNw+IhjCS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamab3/Basics-of-Speech-Recognition/blob/main/Transcribing_Audio_with_Google_Web_Speech_Speech_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries"
      ],
      "metadata": {
        "id": "6nSEC0I-tZOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Library for audio analysis\n",
        "import librosa\n",
        "import librosa.display\n",
        "import soundfile as sf\n",
        "import speech_recognition as sr\n",
        "\n",
        "from jiwer import wer, cer\n",
        "from IPython.display import Audio\n",
        "\n",
        "import whisper\n",
        "\n",
        "import csv\n",
        "import os\n",
        "import tempfile\n",
        "import wave\n",
        "\n",
        "from gtts import gTTS"
      ],
      "metadata": {
        "id": "lpUj0zHetcym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJjsvnsO2tbS"
      },
      "outputs": [],
      "source": [
        "# Loading the audio file\n",
        "audio_signal, sample_rate = librosa.load('speech_01.wav', sr=None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_rate"
      ],
      "metadata": {
        "id": "j4AFYUrLxgJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diplaying the audio by plotting amplitude with time using matplotlib\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.waveshow(audio_signal, sr=sample_rate)\n",
        "plt.title('Waveform')\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.show()\n",
        "\n",
        "# To hear the file as well.\n",
        "Audio('speech_01.wav')"
      ],
      "metadata": {
        "id": "Z6wxw0GZxhr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using SpeechRecognition Library: Google Web Speech API"
      ],
      "metadata": {
        "id": "yuZbPaGD2-Ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recognizer = sr.Recognizer()"
      ],
      "metadata": {
        "id": "qtRz2IpA29uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'speech_01.wav'"
      ],
      "metadata": {
        "id": "vrbG9JrQyd7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to transcribe audio to text\n",
        "def transcribe_audio(file_path):\n",
        "  with sr.AudioFile(file_path) as source:\n",
        "    audio_data = recognizer.record(source)\n",
        "    text = recognizer.recognize_google(audio_data)\n",
        "    print(text)\n",
        "    return text\n",
        "\n",
        "transcribed_text = transcribe_audio(file_path)"
      ],
      "metadata": {
        "id": "edhYruVR3-Bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Error Rate (WER) and Character Error Rate (CER)"
      ],
      "metadata": {
        "id": "mgEKekek5h1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth = \"\"\"My name is Ivan and I am excited to have you as part of our learning community!\n",
        "Before we get started, I’d like to tell you a little bit about myself. I’m a sound engineer turned data scientist,\n",
        "curious about machine learning and Artificial Intelligence. My professional background is primarily in media production,\n",
        "with a focus on audio, IT, and communications\"\"\""
      ],
      "metadata": {
        "id": "UoE5hVN85ofx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculated_wer = wer(ground_truth, transcribed_text)\n",
        "calculated_cer = cer(ground_truth, transcribed_text)\n",
        "print(f\"Word Error Rate (WER): {calculated_wer:.4f}\")\n",
        "print(f\"Character Error Rate (CER): {calculated_cer:.4f}\")"
      ],
      "metadata": {
        "id": "yANU7dTMAB4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's remove the noise. In order to do it, let's convert amplitude-time graph to frequency domain graph to obtain Spectrogram"
      ],
      "metadata": {
        "id": "oJQirDwpDUBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "S = librosa.stft(audio_signal)"
      ],
      "metadata": {
        "id": "IrhIQUTnDTil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "S_db = librosa.amplitude_to_db(abs(S), ref = np.max)"
      ],
      "metadata": {
        "id": "BmXj2RfLKrjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensuring the maximum amplitude is set to zero\n",
        "np.max(S_db)"
      ],
      "metadata": {
        "id": "XNLIC3IwK0C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the spectrogram\n",
        "plt.figure(figsize=(12,4))\n",
        "librosa.display.specshow(data=S_db, sr = sample_rate, x_axis='time', y_axis='log')\n",
        "plt.colorbar(format = '%+2.0f db')\n",
        "plt.title('Spectrogram')\n",
        "plt.xaxis('Time')\n",
        "plt.yaxis('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pCeG6VJhLA4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dealing with Background Noise"
      ],
      "metadata": {
        "id": "po5lvEH_NJcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# applying low frequencies cut-off filter\n",
        "signal_filtered = librosa.effects.preemphasis(audio_signal, coef = 0.97)\n",
        "sf.write('filtered_speech_01.wav', signal_filtered, sample_rate)\n",
        "output_file = 'filtered_speech_01.wav'"
      ],
      "metadata": {
        "id": "OKCbo0hcNLUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Play the original file\n",
        "print('Playing the original audio:')\n",
        "Audio(file_path)"
      ],
      "metadata": {
        "id": "i1-Fo96RPepO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Play the filtered file\n",
        "print('Playing the filtered audio:')\n",
        "Audio(output_file)"
      ],
      "metadata": {
        "id": "tvRNo1yJPs3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "S = librosa.stft(signal_filtered)\n",
        "S_db = librosa.amplitude_to_db(abs(S), ref = np.max)\n",
        "\n",
        "# plot the spectrogram\n",
        "plt.figure(figsize=(12,4))\n",
        "librosa.display.specshow(data=S_db, sr = sample_rate, x_axis='time', y_axis='log')\n",
        "plt.colorbar(format = '%+2.0f db')\n",
        "plt.title('Spectrogram')\n",
        "plt.xaxis('Time')\n",
        "plt.yaxis('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "go_qKo3pQCT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcribed_text_filtered = transcribe_audio('filtered_speech_01.wav')"
      ],
      "metadata": {
        "id": "jBjL7ceIQtba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantitative assessment of the results\n",
        "calculated_wer = wer(ground_truth, transcribed_text_filtered)\n",
        "calculated_cer = cer(ground_truth, transcribed_text_filtered)\n",
        "print(f\"Word Error Rate (WER): {calculated_wer:.4f}\")\n",
        "print(f\"Character Error Rate (CER): {calculated_cer:.4f}\")"
      ],
      "metadata": {
        "id": "4J3BbEaPREqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transcribing Audio with OpenAI's Whisper"
      ],
      "metadata": {
        "id": "-BX7KI-bMeLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"base\")"
      ],
      "metadata": {
        "id": "PGuXR_jOMuHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.transcribe(file_path)"
      ],
      "metadata": {
        "id": "dwkI6M35Govh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcribed_text_whisper = result['text']\n",
        "transcribed_text_whisper"
      ],
      "metadata": {
        "id": "i2fMsFvNGy2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result['language']"
      ],
      "metadata": {
        "id": "k2MP59_vHIcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantitative assessment of the results\n",
        "calculated_wer = wer(ground_truth, transcribed_text_whisper)\n",
        "calculated_cer = cer(ground_truth, transcribed_text_whisper)\n",
        "print(f\"Word Error Rate (WER): {calculated_wer:.4f}\")\n",
        "print(f\"Character Error Rate (CER): {calculated_cer:.4f}\")"
      ],
      "metadata": {
        "id": "pcU0E6JfHOg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transcribing multiple audio files from a directory"
      ],
      "metadata": {
        "id": "_hGtb1j7IHln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory_path = \"C:/Users/Downloads/Speech Recognition\""
      ],
      "metadata": {
        "id": "EZH2JgBuINf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_directory_whisper(directory_path):\n",
        "  transcriptions = []\n",
        "  for file_name in os.listdir(directory_path):\n",
        "    if file_name.endswith(\".wav\"):\n",
        "      files_path = os.path.join(directory_path, file_name)\n",
        "      result = model.transcribe(files_path)\n",
        "      transcription = result[\"text\"]\n",
        "      transcriptions.append({\"file_name\": file_name, \"transcription\": transcription})\n",
        "  return transcriptions"
      ],
      "metadata": {
        "id": "vX2gPM7kIe1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcriptions = transcribe_directory_whisper(directory_path)\n",
        "transcriptions"
      ],
      "metadata": {
        "id": "CQJQ6QDLJh1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving audio transcriptions to csv file"
      ],
      "metadata": {
        "id": "5l4vhuQsKCn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = \"transcriptions.csv\"\n",
        "\n",
        "# Opening the file\n",
        "with open(output_file, mode = \"w\", newline=\"\") as file:\n",
        "  writer = csv.writer(file)\n",
        "  writer.writerow([\"Track Number\", \"File Name\", \"Transcription\"])\n",
        "  for number, transcription in enumerate(transcriptions, start = 1):\n",
        "    writer.writerow([number, transcription[\"file_name\"], transcription[\"transcription\"]])"
      ],
      "metadata": {
        "id": "tR5UCMy4KF7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reversing the process: Text to Speech"
      ],
      "metadata": {
        "id": "LkF7Avr-Oj_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\" Thank you for taking the time out to learn about speech recognition! I'm excited. But this concludes our lesson. See you soon! \"\"\"\n",
        "\n",
        "tts = gTTS(text = text, lang = 'en')\n",
        "tts.save(\"output.mp3\")\n",
        "\n",
        "os.system(\"start output.mp3\")"
      ],
      "metadata": {
        "id": "6bzVuCnwLwau"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}